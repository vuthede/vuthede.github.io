I"x<h3 id="1-introduction">1. Introduction</h3>
<p>We all know that there are 2 main problems when training deep learning model which are overfitting and underfitting problems. Each problem have different solutions to tackle. e.g</p>
<ul>
  <li>With overfitting: Add more data, data augmentation, reduce complexity of model,etc</li>
  <li>With underfitting: Using bigger model, double-check some bugs (e.g inputâ€™s unnormalized, bug in loss function causing Nan value, etc)</li>
</ul>

<h3 id="2-my-case">2. My case</h3>
<p>At the beginning of my project, I train the eyegaze model and got the underfitting problem, the training loss looked like the image below.</p>

<image here="">

**Let's together debug it**.

Firstly, I check the above-mentioned solutions:
- Using bigger model. From mobilev2 --&gt; resnet50
- Check if input image is normalized. Yes it is normalized. 
- Loss function I used was **l2**, and it was obvious that the loss values are the _normal_ float number, there is no Nan value here.
    $$ lossgaze = \sqrt(pred - gt)^2 $$

Sadly, it did not fix the issue. So..., what to do next? I thought there may be **inconsitence** or **corruption in the dataset** like wrong labels or stuff like that, which might lead to model confusing and difficult to learn. 

Yeah it might be, then I decided to use a very small subdet of dataset (around only 100 samples), which I manually double-checked the quality, for training to see whether the model can even overfit it or not.  _This technique is mentioned on the training model recipe of Karpathy [overfit one batch](http://karpathy.github.io/2019/04/25/recipe/)_.

Unfortunately, It still does not work, the training loss did not even decrease! So..., what next? Uptil now, it seems there is nothing wrong with the _forward pass_ from data, label and loss function. What if there is any wrong with the _backward pass_ , which actually is **gradient**. Yeah, let's check it. 

```python

from torch.utils.tensorboard import SummaryWriter
import numpy as np
import torch

class TensorBoardLogger():
    def __init__(self, root="./", experiment_name="experiment1"):
        self.root = root
        self.experiment_name = experiment_name
        self.writer = SummaryWriter(f'{root}/{experiment_name}')
    
    def log(self, metric="loss", value=0.0, step=0):
        self.writer.add_scalar(metric, value, step)
    
    def log_hist(self, tag="gradient_1", value=0.0, step=0):
        self.writer.add_histogram(tag, value, step)

def track_model_gradients(model, tensorboardLogger, step):
    for i, (tag, parm) in enumerate(model.named_parameters()):
        try:
            tensorboardLogger.log_hist(tag=f"{tag}", value=parm.grad.data.cpu().numpy(), step=step)
        except:
            pass
        
        try:
            tensorboardLogger.log_hist(tag=f"weight_{tag}", value=parm.data.cpu().numpy(), step=step)
        except:
            pass

```





</image>
:ET