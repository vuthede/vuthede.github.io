I"3<h3 id="1-introduction">1. Introduction</h3>
<p>We all know that there are 2 main problems when training deep learning model which are overfitting and underfitting problems. Each problem have different solutions to tackle. e.g</p>
<ul>
  <li>With overfitting: Add more data, data augmentation, reduce complexity of model,etc</li>
  <li>With underfitting: Using bigger model, double-check some bugs (e.g inputâ€™s unnormalized, bug in loss function causing Nan value, etc)</li>
</ul>

<h3 id="2-my-case">2. My case</h3>
<p>At the beginning of my project, I train the eyegaze model and got the underfitting problem, the training loss looked like the image below.</p>

<image here="">

**Let's together debug it**.

Firstly, I check the above-mentioned solutions:
- Using bigger model. From mobilev2 --&gt; resnet50
- Check if input image is normalized. Yes it is normalized. 
- Loss function I used was **l2**, and it was obvious that the loss values are the _normal_ float number, there is no Nan value here.
    $$ lossgaze = \sqrt(pred - gt)^2 $$

Sadly, it did not fix the issue. So..., what to do next? I thought there may be **inconsitence** or **corruption in the dataset** like wrong labels or stuff like that, which might lead to model confusing and difficult to learn. 

Yeah it might be, then I decided to use a very small subdet of dataset (around only 100 samples), which I manually double-checked the quality, for training to see whether the model can even overfit it or not.  _This technique is mentioned on the training model recipe of Karpathy [overfit one batch](http://karpathy.github.io/2019/04/25/recipe/)_.

Unfortunately, It still does not work, the training loss did not even decrease! So..., what next? Uptil now, it seems there is nothing wrong with the _forward pass_ from data, label and loss function. 




</image>
:ET